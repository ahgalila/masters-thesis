\subsection{Research Objective} \label{sec:introduction-research-objective-scope-research-objective}

Knowledge sharing will be empirically demonstrated in this thesis by training deep neural networks to perform basic arithmetic operations (such as addition, subtraction, multiplication and division) on images of handwritten digits. These models will accept the images as inputs and will output the result of the operation as vectors that encode the value of the output. The models will also accept a symbolic channel that emulates shared knowledge between learning agents. Training will be performed with and without the presence of symbols. The results will then be analyzed to show the impact of the symbolic channel on the accuracy of the models developed.

Besides showing that the presence of symbols increases the accuracy of artificial neural networks, we also want to empirically explain why symbols allow for this improvement. In deep neural networks, layers of the architecture learn abstractions that can be shared among different tasks\cite{Bengio:2009:LDA:1658423.1658424}. We believe that some of these abstractions can be learned more effectively from clear symbols than they can from noisy inputs. This leads to better models than if the networks were trained using noisy handwritten digits alone.