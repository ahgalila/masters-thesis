\subsection{Scope} \label{sec:introduction-research-objective-scope-scope}

We focus our research on understanding the effect of symbols on the accuracy of deep neural networks, we also examine the effectiveness of different approaches to encode and provide symbols to our neural network models. Several models are developed to investigate the effects of symbols on training accuracy and to a lesser extent on the speed of training. This thesis presents the results obtained from these experiments as well as an analysis of these results. We limit our experiments to developing neural network architectures using recurrent neural networks based on Long Short Term Memory units (LSTMs). The application domain is also limited to training the LSTM networks to learn to perform addition, subtraction, multiplication and division on images of handwritten digits from the MNIST dataset (see Section \ref{sec:theory-approach-the-mnist-dataset}).

Other types of neural network architectures, such as Convolutional Neural Networks, will not be considered for this problem. MNIST images are preprocessed so that all digits are centered and are of the same orientation and scale. It has been shown that traditional feed-forward networks can perform very well on the MNIST dataset without the need for convolution. Therefore, in order to reduce complexity and focus on the problem at hand, convolutional neural networks are not used. 