\section{Learning with Symbols} \label{sec:introduction-learning-symbols}

Humans generally don't need as many training examples to learn a specific concept or task compared to machine learning systems. For example, a human learner can learn to identify an object in an image using only a single example. When we contrast that with an artificial neural network, the neural network might need hundreds of examples to achieve the same level of accuracy. We as humans build on the knowledge we've accumulated over our lifetimes\cite{Thrun1998}. Designing deep learning systems that accumulate and integrate knowledge over time and over multiple tasks is therefore important to overcome the challenge of learning from impoverished datasets\cite{silver2013lifelong}.

When considering how learning in humans works, it is believed that the ability of individual humans to learn new concepts accurately is hampered by the number and variety of examples of the concepts they are each exposed to. However, this difficulty is overcome when individuals communicate concise examples of concepts between each other using a common language\cite{DBLP:journals/corr/abs-1203-2990}. This common language, which we refer to in this thesis as symbols, is very effective at transferring the knowledge, that has accumulated in a society over a long period of time, from one generation to the next.

Deep artificial neural networks experience similar challenges when learning new concepts from limited datasets. This can be attributed to the existence of many local minima in the hypothesis space of the network's loss function\cite{Larochelle:2009:EST:1577069.1577070}. We therefore hypothesize that similar to how human learners use symbols to overcome difficulty in learning from limited real examples, artificial neural networks can also benefit from symbols to improve the effectiveness (accuracy) of training using a limited dataset.

The research presented in this thesis empirically investigates the effect of introducing a clear and consistent symbolic channel on the accuracy of neural network models. We propose that the clear symbols introduced during training introduce inductive bias to the network which guides the training algorithm to a more optimal representation\cite{Thrun1998}. The next section states the objective and scope of our research.