\subsection{Multi-Task Learning} \label{sec:theory-hypothesis-multi-task-learning}

The insight for our hypothesis comes from our understanding of multi-task learning. In multi-task learning, a deep neural network model is trained to learn several different but related tasks together\cite{silver2007context}. Section \ref{sec:background-deep-learning} introduced deep learning and explained how deep neural networks are constructed from several layers of artificial neurons that learn levels of abstract representation of the input values. When learning multiple tasks, certain abstractions are shared among the different tasks, therefore reducing the overall number of training examples that are needed to learn any one task.

By incorporating a clear symbol channel into our networks, the deep architecture has a better chance of developing useful abstract representations from the noisy inputs\cite{Bengio:2009:LDA:1658423.1658424}. Since learning using the clear symbols is easier than learning from the noisy inputs, these common abstractions will be discovered early in the training process making the ability of the network to converge on a solution much easier, therefore improving the effectiveness of training.