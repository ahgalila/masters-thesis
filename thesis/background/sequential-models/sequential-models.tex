\section{Sequence Modeling} \label{sec:background-sequence-modeling} 

Machine learning systems face many problems where they must consider a sequence of inputs on one or more sensory channels. That is, their predictive model is dependent on a sequence of data points. For example, an autonomous vehicle that learns to move by analyzing sequences of frames from a video, depends on previous frames in the input sequence to infer context. It is therefore important to extend the traditional neural network models to consider temporal information as well as spatial information\cite{DBLP:journals/corr/Lipton15}.

One approach to this type of problem is to explicitly capture sequential information or context by concatenating the inputs by a specific number of its predecessors or successors. This effectively creates a sliding window that scans the sequential data\cite{DBLP:journals/corr/Lipton15}. An example of this approach is seen in the work done by Mikolov et al. 2013 where the dimensionality of a corpus of text represented using the bag of words technique (words from a corpus are represented using a sparse vector where each feature of the vector corresponds to a word and the occurrence of a specific word is expressed by setting the word's corresponding feature in the vector) is considerably reduced to a much smaller vector and at the same time maintaining context between words such that words that often appear in the same context would have vectors in close proximity to one another\cite{DBLP:journals/corr/abs-1301-3781}. The drawback to this approach however is that the length of the sequence is fixed and therefore the effect of time on the inputs is limited to some extent.

Certain applications rely on having the ability to model arbitrarily long sequences of inputs. In this section we introduce a type of neural network models called recurrent neural networks (RNNs). This type of architecture allows some inputs to be represented as a function of previous outputs and can learn how much influence these previous outputs can have on the current output\cite{DBLP:journals/corr/Lipton15}. We also introduce a variation of RNNs that overcome a major limitation known as the vanishing gradient problem.

\input{background/sequential-models/recurrent-neural-networks/recurrent-neural-networks}

\input{background/sequential-models/long-short-term-memory-units/long-short-term-memory-units}