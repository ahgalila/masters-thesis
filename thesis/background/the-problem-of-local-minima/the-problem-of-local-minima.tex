\section{The Problem of Local Minima} \label{sec:background-problem-local-minima}

Combining many layers of features together to form a neural network may increase the expressiveness and accuracy of the model, however, it adds challenges to the training process. A major challenge for the training algorithm is to search for a global minimum in the hypothesis space, given the presence of many local minima in hypothesis spaces with lower dimensions\cite{Larochelle:2009:EST:1577069.1577070} and the presence of many saddle points and plateaus in higher dimensions\cite{DBLP:journals/corr/DauphinPGCGB14}. Figure \ref{fig:gradient-descent} depicts the training set loss as a function of the weights. The complex topology of the feed forward network adds regions in the loss function that act as local minima. In the context of a small dataset, this makes it more likely for the backpropagation algorithm to converge the weights to one of these local minima instead of the desired global minimum\cite{Larochelle:2009:EST:1577069.1577070}. Overcoming this difficulty in training can be achieved by increasing the number of training examples used. Other techniques involve pre-training the network using unsupervised learning first before using the network as a classifier as is the case with deep belief networks\cite{Larochelle:2009:EST:1577069.1577070}.

When comparing machine learning to how we as humans learn, we find that humans also suffer for the presence of effective local minima that hamper their ability to learn from small datasets\cite{Larochelle:2009:EST:1577069.1577070}. In the next chapter, we present our theory that considers how this problem can be overcome in artificial neural networks by introducing a symbolic channel. This symbol channel emulates how the concise knowledge shared by human learners helps individual humans overcome otherwise impoverished training data. The symbolic channel introduces bias that favors regions of the hypothesis space avoiding local minima and allowing the gradient descent process to converge to a more suitable minimum.