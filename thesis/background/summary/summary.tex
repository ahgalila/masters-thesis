\section{Summary} \label{sec:background-summary}

This chapter provided a detailed background on artificial neural networks that is required by the reader to understand the problem that forms the basis of this thesis and the experiments that are used to support the theory. It explains the basic building block of neural networks, the perceptron. It then discusses how more complex models can be built from the perceptron and the advantages of deep neural networks. Next, we described a form of neural network that can model sequences of examples called a recurrent neural network and specifically a variation that uses LSTMs. This type of neural network is used extensively in our experiments as described in Chapter 4. Finally, we showed how impoverished training sets can lead to the problem of local minima which is the underlying problem being investigated in this thesis.

In the next chapter, we reintroduce the local minima problem in the context of learning from impoverished datasets. We show that it is difficult to overcome this problem with the limited data and we propose that by introducing clearer and more consistent symbolic information, we allow the model to select a hypothesis space that is easier for the gradient descent algorithm to navigate, thereby produce more accurate models.